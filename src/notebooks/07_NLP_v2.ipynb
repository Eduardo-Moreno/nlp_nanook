{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = pd.read_pickle('../../data/preprocessed/stemm_lemm_text_nanook.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean[['Page Name', 'Message', 'Message_clean', 'Message_clean_stemm', 'Message_clean_lemm']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(df_clean['Facebook Id']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El set de datos consta de $87,422$ posts de Facebook realizados por $20,415$ usuarios."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se han procesado los posts, a continuación se muestran los posts:\n",
    "- Originales\n",
    "- Limpios (sin *stop words*)\n",
    "- *Lematizados*\n",
    "- *Stemmizados*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ORIGINAL MESSAGE')\n",
    "for i, text in enumerate(df_clean['Message']):\n",
    "    print(f\"Post {i + 1}: {text}\")\n",
    "    #display(Markdown(f\"Post {i + 1}: {text}\"))\n",
    "    if i == 4:\n",
    "        break\n",
    "print(\"\\n\")\n",
    "\n",
    "print('CLEAN MESSAGE')\n",
    "for i, text in enumerate(df_clean['Message_clean']):\n",
    "    print(f\"Post {i + 1}: {text}\")\n",
    "    if i == 4:\n",
    "        break\n",
    "print(\"\\n\")\n",
    "\n",
    "print('LEMMED MESSAGE')\n",
    "for i, text in enumerate(df_clean['Message_clean_lemm']):\n",
    "    print(f\"Post {i + 1}: {text}\")\n",
    "    if i == 4:\n",
    "        break\n",
    "print(\"\\n\")\n",
    "\n",
    "print('STEMMED MESSAGE')\n",
    "for i, text in enumerate(df_clean['Message_clean_stemm']):\n",
    "    print(f\"Post {i + 1}: {text}\")\n",
    "    if i == 4:\n",
    "        break\n",
    "print(\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar todos los posts en un archivo de texto plano (`facebook_posts.txt`) para procesarlos posteriormente con `spaCy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_FACEBOOK_POSTS_PATH_FILE = \"../../data/preprocessed/clean_posts.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 0:\n",
    "    with open(CLEAN_FACEBOOK_POSTS_PATH_FILE, \"w\") as f:\n",
    "        for i, post in enumerate(df_clean['Message_clean'].values.tolist()):\n",
    "            f.write(post + \"\\n\\n\")\n",
    "    print('Se escribireron {:,} posts de facebook en el archivo de texto plano {:}.'.format(i, CLEAN_FACEBOOK_POSTS_PATH_FILE.split('/')[-1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `spaCy`\n",
    "\n",
    "`spaCy` es una librería de Python utilizada para proyectos de **Procesamiento del Lenguaje Natural** (NLP) poniendo a la disposición del programador varias técnicas utilizadas en un pipeline de NLP:\n",
    "- Tokenization\n",
    "- Normalización del texto (eliminar mayúsculas, stemming, lemmatization)\n",
    "- Part-Of-Speech tagging\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "Para instalar `spaCy` y dependencias:\n",
    "```\n",
    "pip install spacy\n",
    "\n",
    "pip install es-core-news-sm\n",
    "\n",
    "python -m spacy download es_core_news_lg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import itertools as it\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load('es_core_news_sm')\n",
    "nlp = spacy.load('es_core_news_lg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostremos un posts de facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(CLEAN_FACEBOOK_POSTS_PATH_FILE, encoding='utf_8') as f:\n",
    "    sample_review = list(it.islice(f, 4, 5))[0]\n",
    "    sample_review = sample_review.replace('\\\\n', '\\n')\n",
    "        \n",
    "print(sample_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora trabajemos con `spaCy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_review = nlp(sample_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_review)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luce idéntico a la cadena sin utilizar `spaCy`, ¿cuál es la diferencia?\n",
    "\n",
    "Gracias a `spaCy` es fácil obtener la tarea de preprocesamiento (realizada con anterioridad), así como más herramientas útiles en **NLP**:\n",
    "- Part-of-Speech (POS)\n",
    "- Lemmatization\n",
    "- Stop words\n",
    "- Named Entity Recognition (adjetivos, sustantivos, pronombres, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = [token.orth_ for token in parsed_review]\n",
    "token_pos = [token.pos_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_pos),\n",
    "             columns=['token_text', 'part_of_speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, entity in enumerate(parsed_review.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity, '-', entity.label_)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_review]\n",
    "token_shape = [token.shape_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_lemma, token_shape),\n",
    "             columns=['token_text', 'token_lemma', 'token_shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, sentence in enumerate(parsed_review.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_review]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_review]\n",
    "\n",
    "pd.DataFrame(zip(token_text, token_entity_type, token_entity_iob),\n",
    "             columns=['token_text', 'entity_type', 'inside_outside_begin'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Una de las ventajas de trabajar con `spaCy` es que podemos preprocesar los datos **más rápido**!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num)\n",
    "                    for token in parsed_review]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           ])\n",
    "\n",
    "df.loc[:, 'stop?':'number?'] = df.loc[:, 'stop?':'number?'].map(lambda x: 'Yes' if x else '')\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def line_review(filename):\n",
    "    \"\"\"\n",
    "    generator function to read in reviews from the file\n",
    "    and un-escape the original line breaks in the text\n",
    "    \"\"\"\n",
    "    \n",
    "    with codecs.open(filename, encoding='utf_8') as f:\n",
    "        for review in f:\n",
    "            yield review.replace('\\\\n', '\\n')\n",
    "            \n",
    "def lemmatized_sentence_corpus(filename):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    \n",
    "    for parsed_review in nlp.pipe(line_review(filename),batch_size=10000, n_process=4):\n",
    "        for sent in parsed_review.sents:\n",
    "            yield u' '.join([token.lemma_ for token in sent if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEMM_FACEBOOK_POSTS_PATH_FILE = '../../data/preprocessed/lemm_posts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# this is a bit time consuming - make the if statement True\n",
    "# if you want to execute data prep yourself.\n",
    "if 1 == 1:\n",
    "    with codecs.open(LEMM_FACEBOOK_POSTS_PATH_FILE, 'w', encoding='utf_8') as f:\n",
    "        for sentence in lemmatized_sentence_corpus(CLEAN_FACEBOOK_POSTS_PATH_FILE):\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "tokens_spacy = []\n",
    "for post in tqdm(facebook_post_clean[:4000]):\n",
    "    doc = nlp(post)\n",
    "    token_i = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    tokens_spacy.append(token_i)\n",
    "\n",
    "print(tokens_spacy[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens_spacy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Es necesaria la versión `1.10.1` de `scipy`:\n",
    "\n",
    "```\n",
    "pip install scipy==1.10.1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(tokens_spacy, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[tokens_spacy], threshold=100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trigram_mod[bigram_mod[tokens_spacy[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod[tokens_spacy[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_bigrams = make_bigrams(tokens_spacy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_bigrams\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimum number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "limit=40\n",
    "start=2\n",
    "step=6\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "for num_topics in tqdm(range(start, limit, step)):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                id2word=id2word,\n",
    "                                                num_topics=num_topics, \n",
    "                                                random_state=100,\n",
    "                                                update_every=1,\n",
    "                                                chunksize=100,\n",
    "                                                passes=10,\n",
    "                                                alpha='auto',\n",
    "                                                per_word_topics=True)\n",
    "    model_list.append(lda_model)\n",
    "    coherencemodel = CoherenceModel(model=lda_model, texts=data_words_bigrams, dictionary=id2word, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_top, coh_val in zip(x, coherence_values):\n",
    "    print(f\"{n_top}: {coh_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=14, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nanook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
