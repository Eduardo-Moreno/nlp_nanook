{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from nltk.corpus import cess_esp\n",
    "from collections import Counter\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw')  # Descargar el recurso WordNet en español\n",
    "#nltk.download('cess_esp')  # Descargar el corpus CESS en español (etiquetado POS)\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '/Users/eduardomorenoortiz/Desktop/ITAM/nanook/nlp_nanook/src') # LOCAL\n",
    "from utils.utils import get_corpus_N_gram"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual configuration\n",
    "\n",
    "In case `nltk.download('omw')`, `nltk.download('cess_esp')` and/or `nltk.download('punkt')` response were **False**: download following models from the next [link](http://www.nltk.org/nltk_data/):\n",
    "- Model: *Extended Open Multilingual WordNet*, ID: *extended_omw*\n",
    "- Model: *CESS-ESP Treebank*, ID: *cess_esp*\n",
    "- Model: *Punkt Tokenizer Models*, ID: *punkt*\n",
    "- Model: *WordNet*, ID: *wordnet*\n",
    "\n",
    "Once files are downloaded move them to:\n",
    "- *extended_omw*, *cess_esp*: */env_nanook/lib/nltk_data/tokenizers* \n",
    "- *punkt*: */env_nanook/lib/nltk_data/tokenizers*\n",
    "- *wordnet*: */env_nanook/lib/nltk_data/tokenizers/corpora*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/Users/eduardomorenoortiz/Desktop/ITAM/nanook/nlp_nanook/env_nlp_nanook/lib/python3.11/site-packages/nltk/nltk_data') # Uncomment if necessary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean = pd.read_pickle('../../data/preprocessed/clean_text_nanook.pkl') # Uncomment if first time running the notebook\n",
    "df_clean = pd.read_pickle('../../data/preprocessed/stemm_lemm_text_nanook.pkl') # Comment if first time running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "**Stemming** is a text normalization process used in *Natural Language Processing* (*NLP*) to reduce words to their base or root form, known as the **stem**. The goal of stemming is to obtain a common representation for variations of words that share the same root, even if that root may not be an actual word. The resulting stems may not always be semantically valid words but serve the purpose of grouping similar words together.\n",
    "\n",
    "Stemming involves removing prefixes or suffixes from words to derive their root forms. The idea is to simplify words to their basic linguistic or morphological components. This process is particularly useful in tasks such as text analysis, information retrieval, and search engine optimization.\n",
    "\n",
    "Key points about stemming:\n",
    "\n",
    "1. Reduction of Inflected Words: Stemming reduces words to their base or root forms, removing variations caused by different tenses, pluralization, or other grammatical forms. For example, the stem of \"running\" is \"run,\" and the stem of \"happily\" is \"happi.\"\n",
    "\n",
    "1. Heuristic-Based Approach: Stemming algorithms typically use heuristic rules to apply transformations to words. These rules are designed to strip common prefixes or suffixes, but they may not always result in a linguistically valid word.\n",
    "\n",
    "1. Simplification of Vocabulary: Stemming helps simplify the vocabulary by treating different inflections of a word as the same entity. This can reduce the dimensionality of the data and improve the efficiency of text analysis.\n",
    "\n",
    "1. Fast and Lightweight: Stemming is computationally less intensive than lemmatization, making it faster and more suitable for tasks where speed is crucial.\n",
    "\n",
    "Here's a simple example in English:\n",
    "\n",
    "- Word: \"Running\"\n",
    "- Stem: \"Run\"\n",
    "\n",
    "It's important to note that stemming does not consider the context or semantics of words. Different stemming algorithms may produce different results, and there might be cases where stemming produces stems that are not valid words or may not accurately reflect the intended meaning. Popular stemming algorithms include the Porter Stemmer and the Lancaster Stemmer.\n",
    "\n",
    "Reference:\n",
    "- Prompt: What is stemming? - [ChatGPT](https://chat.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def realizar_stemming(texto):\n",
    "    palabras = nltk.word_tokenize(texto, language='spanish')\n",
    "    stems = [stemmer.stem(palabra) for palabra in palabras]\n",
    "    return ' '.join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original text: {df_clean['Message_clean'][0]}\")\n",
    "print(f\"Stemmed text: {realizar_stemming(texto=df_clean['Message_clean'][0])}\")\n",
    "print('-'*30)\n",
    "print(f\"Original text: {df_clean['Message_clean'][23]}\")\n",
    "print(f\"Stemmed  text: {realizar_stemming(texto=df_clean['Message_clean'][23])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming = df_clean['Message_clean'].apply(lambda x: realizar_stemming(texto=x)) # Uncomment if first time running the notebook\n",
    "#df_clean['Message_clean_stemm'] = stemming # Uncomment if first time running the notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "**Lemmatization** is a linguistic process commonly used in *Natural Language Processing* (*NLP*) to reduce words to their base or root form, known as the **lemma**. The lemma represents the canonical, dictionary form of a word. Lemmatization is different from stemming, which involves removing prefixes or suffixes from a word to obtain its root form, even if that root form may not be an actual word.\n",
    "\n",
    "The main goal of lemmatization is to group different inflected forms of a word so they can be analyzed as a single item. This process is crucial for tasks like text analysis, information retrieval, and language modeling. Lemmatization helps in reducing the dimensionality of the vocabulary and improving the accuracy of text analysis by focusing on the core meaning of words.\n",
    "\n",
    "Here are some key points about lemmatization:\n",
    "\n",
    "1. Word Normalization: Lemmatization performs a kind of word normalization by transforming words into their base or root form. For example, the lemma of the word \"running\" is \"run,\" and the lemma of \"better\" is \"good.\"\n",
    "\n",
    "1. Context Preservation: Unlike stemming, lemmatization considers the context and meaning of a word before determining its base form. This helps in producing more accurate and meaningful results.\n",
    "\n",
    "1. Dictionary-Based Approach: Lemmatization often relies on dictionaries or morphological analysis to map words to their lemmas. These dictionaries include information about the base forms of words and their grammatical properties.\n",
    "\n",
    "1. Part-of-Speech Consideration: Lemmatization may take into account the part of speech (POS) of a word to determine its correct lemma. For example, the lemma of \"better\" as an adjective is \"good,\" but as an adverb, it remains \"better.\"\n",
    "\n",
    "1. Improved Text Analysis: Lemmatization can improve the accuracy of text analysis tasks such as sentiment analysis, topic modeling, and information retrieval by reducing words to their essential forms.\n",
    "\n",
    "Here's a simple example in English:\n",
    "\n",
    "- Word: \"Running\"\n",
    "- Lemma: \"Run\"\n",
    "\n",
    "In the context of NLP, libraries like NLTK (Natural Language Toolkit) and spaCy provide lemmatization tools and resources for multiple languages. Lemmatization is a valuable preprocessing step in many NLP applications to enhance the understanding and analysis of textual data.\n",
    "\n",
    "\n",
    "Reference:\n",
    "- Prompt: What is lemmatization? - [ChatGPT](https://chat.openai.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "def lematizar_texto(texto):\n",
    "    palabras = nltk.word_tokenize(texto, language='spanish')\n",
    "    lemas = [lematizador.lemmatize(palabra) for palabra in palabras]\n",
    "    return ' '.join(lemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original text: {df_clean['Message_clean'][0]}\")\n",
    "print(f\"Lemmatized  text: {lematizar_texto(texto=df_clean['Message_clean'][0])}\")\n",
    "print('-'*30)\n",
    "print(f\"Original text: {df_clean['Message_clean'][23]}\")\n",
    "print(f\"Lemmatized  text: {lematizar_texto(texto=df_clean['Message_clean'][23])}\")\n",
    "print('-'*30)\n",
    "print(f\"Original text: nota como la palabra graves tiene asociado el token grav en la frase menciono los graves conflictos, pero no para menciono los gravess conflictos\")\n",
    "print(f\"Lemmatized  text: {lematizar_texto(texto='nota como la palabra graves tiene asociado el token grav en la frase menciono los graves conflictos, pero no para menciono los gravess conflictos')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization = df_clean['Message_clean'].apply(lambda x: lematizar_texto(texto=x)) # Uncomment if first time running the notebook\n",
    "#df_clean['Message_clean_lemm'] = lemmatization # Uncomment if first time running the notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../utils/spanish_stopwords.txt', 'r') as archivo:\n",
    "    stop_words = [linea.strip() for linea in archivo]\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para formatear las etiquetas del eje Y en formato abreviado (por ejemplo, 120k)\n",
    "def formato_abreviado(valor, posicion):\n",
    "    if valor >= 1000:\n",
    "        return f'{int(valor/1000)}k'\n",
    "    else:\n",
    "        return int(valor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean.to_pickle('/Users/eduardomorenoortiz/Desktop/repos/cdas_itam_nanook/data/preprocessed/stemm_lemm_text_nanook.pkl') # Uncomment if first time running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_stemm = get_corpus_N_gram(list_text=df_clean['Message_clean_stemm'], stop_words=stop_words, ngram=1, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lemm = get_corpus_N_gram(list_text=df_clean['Message_clean_lemm'], stop_words=stop_words, ngram=1, show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_corpus_stem = Counter(corpus_stemm)\n",
    "count_corpus_lemm = Counter(corpus_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_corpus_stem = count_corpus_stem.most_common()\n",
    "count_corpus_lemm = count_corpus_lemm.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(count_corpus_stem))\n",
    "print(len(count_corpus_lemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = []\n",
    "n_words_stem = []\n",
    "n_words_lem = []\n",
    "for threshold in range(2, 100):\n",
    "    thresholds.append(threshold)\n",
    "    n_words_stem.append(len([palabra for palabra, frecuencia in count_corpus_stem.items() if frecuencia < threshold]))\n",
    "    n_words_lem.append(len([palabra for palabra, frecuencia in count_corpus_lemm.items() if frecuencia < threshold]))\n",
    "\n",
    "for threshold in range(2, 100):\n",
    "    thresholds.append(threshold)\n",
    "    n_words_stem.append(len(set(palabra for palabra, frecuencia in count_corpus_stem.items() if frecuencia < threshold)))\n",
    "    n_words_lem.append(len(set(palabra for palabra, frecuencia in count_corpus_lemm.items() if frecuencia < threshold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold = pd.DataFrame({'threshold': thresholds, 'stemming':n_words_stem, 'lemmatization':n_words_lem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_threshold.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las dos líneas usando seaborn\n",
    "sns.lineplot(data=df_threshold, x='threshold', y='stemming', label='Lost Stemming')\n",
    "sns.lineplot(data=df_threshold, x='threshold', y='lemmatization', label='Lost Lemmatization')\n",
    "# Ajuste de labels en los ejes\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(formato_abreviado))\n",
    "# Threshold = 25\n",
    "plt.axvline(x=25, color='red', linestyle='dashed', label='Threshold = 25')\n",
    "# Ajustes adicionales, como etiquetas y leyenda\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Lost')\n",
    "plt.title('Lost of tokens')\n",
    "plt.legend()\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that may exists some **logarithmic** pattern on both lines!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_words_stem = [palabra for palabra, frecuencia in count_corpus_stem.items() if frecuencia < threshold]\n",
    "lost_words_lemm = [palabra for palabra, frecuencia in count_corpus_lemm.items() if frecuencia < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count_corpus_lemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lost tokens\")\n",
    "print(f\"Stemming: {len(lost_words_stem)}\")\n",
    "print(f\"Lemmatization: {len(lost_words_lemm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stem = [palabra for palabra, frecuencia in count_corpus_stem.items() if frecuencia >= threshold]\n",
    "words_lemm = [palabra for palabra, frecuencia in count_corpus_lemm.items() if frecuencia >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_corpus_stem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus/corpus_counter_stemming.txt', 'w') as f:\n",
    "    for item_i in count_corpus_stem:\n",
    "        f.write(f\"{item_i[0]}: {item_i[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/corpus/corpus_counter_lemmatization.txt', 'w') as f:\n",
    "    for item_i in count_corpus_lemm:\n",
    "        f.write(f\"{item_i[0]}: {item_i[1]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select words that appear at least `threshold` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"threshold: {threshold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stem = [palabra for palabra, frecuencia in count_corpus_stem.items() if frecuencia >= threshold]\n",
    "words_lemm = [palabra for palabra, frecuencia in count_corpus_lemm.items() if frecuencia >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of words by Stemming: {len(words_stem)}\")\n",
    "print(f\"Number of words by Lemmatization: {len(words_lemm)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nanook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
